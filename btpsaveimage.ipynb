{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/x.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ec5c0f16849b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCDR\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#calculated cdr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[0my1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVAL\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#It's actual label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m \u001b[0mx1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/x.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/y.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/a.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3018\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3019\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[1;32m-> 3020\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3022\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    155\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[0;32m    156\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                                      compression=self.compression)\n\u001b[0m\u001b[0;32m    158\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[1;31m# Python 3 and encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m             \u001b[1;31m# Python 3 and no explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/x.csv'"
     ]
    }
   ],
   "source": [
    "from scipy import signal\n",
    "import cv2\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "from PIL import Image\n",
    "import xlrd \n",
    "import math\n",
    "from pylab import*\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "from skimage import data\n",
    "\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(10,10))\n",
    "\n",
    "#IMPORT GROUND TRUTH\n",
    "#Provide the folder link to the ground truth file is the drishti dataset\n",
    "wb = xlrd.open_workbook(\"E://drishti//Drishti-GS1_files//Drishti-GS1_diagnosis.xlsx\") \n",
    "sheet = wb.sheet_by_index(0) \n",
    "val = [sheet.col_values(1)[5:],sheet.col_values(8)[5:]]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "   \n",
    "   \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    # returns an image of dtype int in range [0, 255]\n",
    "    return np.asarray(Image.open(path))\n",
    "\n",
    "#function to load image and their name\n",
    "def load_set(folder, shuffle=False):\n",
    "    img_list = sorted(glob.glob(os.path.join(folder, '*.png')) + \\\n",
    "                      glob.glob(os.path.join(folder, '*.jpg')) + \\\n",
    "                      glob.glob(os.path.join(folder, '*.jpeg')))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(img_list)\n",
    "    data = []\n",
    "    filenames = []\n",
    "    for img_fn in img_list:\n",
    "        img = load_image(img_fn)\n",
    "        data.append(img)\n",
    "        filenames.append(img_fn)\n",
    "    return data, filenames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#DATA EXTRACTION FUNCTION\n",
    "#db_folder = drishti dataset folder\n",
    "#cdr = set it true to get the cdr values of 4 experts\n",
    "#train_data = setting it true gives training data and false gives testing data\n",
    "def extract_DRISHTI_GS_train(db_folder,cdr,train_data):\n",
    "\n",
    "    file_codes_all,exp1,exp2,exp3,exp4 = [], [], [], [], []\n",
    "    if train_data:\n",
    "        set_path = os.path.join(db_folder, 'Drishti-GS1_files','Drishti-GS1_files', 'Training')\n",
    "    else:\n",
    "        set_path = os.path.join(db_folder, 'Drishti-GS1_files','Drishti-GS1_files', 'Test')\n",
    "    images_path = os.path.join(set_path, 'Images')\n",
    "    X_all, file_names = load_set(images_path)\n",
    "    rel_file_names = [os.path.split(fn)[-1] for fn in file_names]\n",
    "    rel_file_names_wo_ext = [fn[:fn.rfind('.')] for fn in rel_file_names]\n",
    "    if train_data:\n",
    "        file_codes = [fn[fn.find('_'):] for fn in rel_file_names_wo_ext]\n",
    "    else:\n",
    "        file_codes = [fn[fn.find('_'):] for fn in rel_file_names_wo_ext]\n",
    "    file_codes_all.extend(file_codes)\n",
    "    \n",
    "    for fn in rel_file_names_wo_ext:\n",
    "        if cdr:\n",
    "            if train_data:\n",
    "                CDR = open(os.path.join(set_path, 'GT', fn,fn + '_cdrValues.txt'),'r')\n",
    "            else:\n",
    "                CDR = open(os.path.join(set_path, 'Test_GT', fn,fn + '_cdrValues.txt'),'r')\n",
    "            CDR = list(CDR)\n",
    "            CDR = CDR[0].split()\n",
    "            exp1.append(CDR[0])\n",
    "            exp2.append(CDR[1])\n",
    "            exp3.append(CDR[2])\n",
    "            exp4.append(CDR[3])\n",
    "            \n",
    "    return X_all, file_codes_all,exp1,exp2,exp3,exp4,file_names\n",
    "    #This functions returns the data images,their names and the corresponding cdr values of each expert in order\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET DATA\n",
    "X_all,file_codes_all,exp1,exp2,exp3,exp4,file_names = extract_DRISHTI_GS_train('E:\\\\drishti\\\\Drishti-GS1_files\\\\Drishti-GS1_files',True,True) #put the folder where dataset is\n",
    "\n",
    "\n",
    "# FUNCTION TO SEGMENT CUP AND DISK\n",
    "#image = fundus image\n",
    "#plot_seg = plots the segmented image\n",
    "#plt_hist = plots the histogram of red and green channel before and after smoothing\n",
    "def segment(image,plot_seg,plot_hist):\n",
    "\n",
    "    image = image[400:1400,500:1600,:] #cropping the fundus image to ger region of interest\n",
    "\n",
    "    Abo,Ago,Aro = cv2.split(image)  #splitting into 3 channels\n",
    "    #Aro = clahe.apply(Aro)\n",
    "    Ago = clahe.apply(Ago)\n",
    "    M = 60    #filter size\n",
    "    filter = signal.gaussian(M, std=6) #Gaussian Window\n",
    "    filter=filter/sum(filter)\n",
    "    STDf = filter.std()  #It'standard deviation\n",
    "    \n",
    "\n",
    "    Ar = Aro - Aro.mean() - Aro.std() #Preprocessing Red\n",
    "    \n",
    "    Mr = Ar.mean()                           #Mean of preprocessed red\n",
    "    SDr = Ar.std()                           #SD of preprocessed red\n",
    "    Thr = 0.5*M - STDf - Ar.std()            #Optic disc Threshold\n",
    "    #print(Thr)\n",
    "\n",
    "    Ag = Ago - Ago.mean() - Ago.std()\t\t #Preprocessing Green\n",
    "    Mg = Ag.mean()                           #Mean of preprocessed green\n",
    "    SDg = Ag.std()                           #SD of preprocessed green\n",
    "    Thg = 0.5*Mg +2*STDf + 2*SDg + Mg        #Optic Cup Threshold\n",
    "    #print(Thg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " hist,bins = np.histogram(Ag.ravel(),256,[0,256])   #Histogram of preprocessed green channel\n",
    "    histr,binsr = np.histogram(Ar.ravel(),256,[0,256]) #Histogram of preprocessed red channel\n",
    "\n",
    "\n",
    "    smooth_hist_g=np.convolve(filter,hist)  #Histogram Smoothing Green\n",
    "    smooth_hist_r=np.convolve(filter,histr) #Histogram Smoothing Red\n",
    "    \n",
    "    #plot histogram if input is true\n",
    "    if plot_hist:\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(hist)\n",
    "        plt.title(\"Preprocessed Green Channel\")\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(smooth_hist_g)\n",
    "        plt.title(\"Smoothed Histogram Green Channel\")\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(histr)\n",
    "        plt.title(\"Preprocessed Red Channel\")\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(smooth_hist_r)\n",
    "        plt.title(\"Smoothed Histogram Red Channel\")\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    r,c = Ag.shape\n",
    "    Dd = np.zeros(shape=(r,c)) #Segmented disc image initialization\n",
    "    Dc = np.zeros(shape=(r,c)) #Segmented cup image initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Using obtained threshold for thresholding of the fundus image\n",
    "    for i in range(1,r):\n",
    "        for j in range(1,c):\n",
    "            if Ar[i,j]>Thr:\n",
    "                Dd[i,j]=255\n",
    "            else:\n",
    "                Dd[i,j]=0\n",
    "\n",
    "    for i in range(1,r):\n",
    "        for j in range(1,c):\n",
    "        \n",
    "            if Ag[i,j]>Thg:\n",
    "                Dc[i,j]=1\n",
    "            else:\n",
    "                Dc[i,j]=0\n",
    "         \n",
    "    #Saving the segmented image in the same place as the code folder      \n",
    "    cv2.imwrite('disk.png',Dd)\n",
    "    plt.imsave('cup.png',Dc)\n",
    "    \n",
    "    if plot_seg:\n",
    "        plt.imshow(Dd, cmap = 'gray', interpolation = 'bicubic')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Optic Disk\")\n",
    "        plt.show()\n",
    "        \n",
    "        plt.imshow(Dc, cmap = 'gray', interpolation = 'bicubic')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Optic Cup\")\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO CALCULATE CDR\n",
    "\n",
    "#import cv2 as cv\n",
    "\n",
    "def cdr(cup,disc,plot,count):\n",
    "    \n",
    "    #morphological closing and opening operations\n",
    "    R1 = cv2.morphologyEx(cup, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(2,2)), iterations = 1)\n",
    "    r1 = cv2.morphologyEx(R1, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7,7)), iterations = 1)\n",
    "    R2 = cv2.morphologyEx(r1, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(1,21)), iterations = 1)\n",
    "    r2 = cv2.morphologyEx(R2, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(21,1)), iterations = 1)\n",
    "    R3 = cv2.morphologyEx(r2, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(33,33)), iterations = 1)\t\n",
    "    r3 = cv2.morphologyEx(R3, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(43,43)), iterations = 1)\n",
    "\n",
    "    img = clahe.apply(r3)\n",
    "    \n",
    "    \n",
    "    ret,thresh = cv2.threshold(cup,127,255,0)\n",
    "    contours,hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE) #Getting all possible contours in the segmented image\n",
    "    cup_diameter = 0\n",
    "    largest_area = 0\n",
    "    el_cup = contours[0]\n",
    "    if len(contours) != 0:\n",
    "        for i in range(len(contours)):\n",
    "            if len(contours[i]) >= 5:\n",
    "                area = cv2.contourArea(contours[i]) #Getting the contour with the largest area\n",
    "                if (area>largest_area):\n",
    "                    largest_area=area\n",
    "                    index = i\n",
    "                    el_cup = cv2.fitEllipse(contours[i])\n",
    "                \n",
    "    cv2.ellipse(img,el_cup,(140,60,150),3)  #fitting ellipse with the largest area\n",
    "    x,y,w,h = cv2.boundingRect(contours[index]) #fitting a rectangle on the ellipse to get the length of major axis\n",
    "    cup_diameter = max(w,h) #major axis is the diameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    #morphological closing and opening operations\n",
    "    R1 = cv2.morphologyEx(disc, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(2,2)), iterations = 1)\n",
    "    r1 = cv2.morphologyEx(R1, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7,7)), iterations = 1)\n",
    "    R2 = cv2.morphologyEx(r1, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(1,21)), iterations = 1)\n",
    "    r2 = cv2.morphologyEx(R2, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(21,1)), iterations = 1)\n",
    "    R3 = cv2.morphologyEx(r2, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(33,33)), iterations = 1)\n",
    "    r3 = cv2.morphologyEx(R3, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(43,43)), iterations = 1)\n",
    "\n",
    "    img2 = clahe.apply(r3)\n",
    "    \n",
    "    ret,thresh = cv2.threshold(disc,127,255,0)\n",
    "    contours,hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE) #Getting all possible contours in the segmented image\n",
    "    disk_diameter = 0\n",
    "    largest_area = 0\n",
    "    el_disc = el_cup\n",
    "    if len(contours) != 0:\n",
    "          for i in range(len(contours)):\n",
    "            if len(contours[i]) >= 5:\n",
    "                area = cv2.contourArea(contours[i]) #Getting the contour with the largest area\n",
    "                if (area>largest_area):\n",
    "                    largest_area=area\n",
    "                    index = i\n",
    "                    el_disc = cv2.fitEllipse(contours[i])\n",
    "                    \n",
    "    cv2.ellipse(img2,el_disc,(140,60,150),3) #fitting ellipse with the largest area\n",
    "    x,y,w,h = cv2.boundingRect(contours[index]) #fitting a rectangle on the ellipse to get the length of major axis\n",
    "    disk_diameter = max(w,h) #major axis is the diameter\n",
    "    cv2.imwrite(\"C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP\\\\images\\\\disk/disk{}.png\".format(count),img)\n",
    "    plt.imsave(\"C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP\\\\images\\\\cup/cup{}.png\".format(count),img2)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " if plot:\n",
    "        plt.imshow(img2, cmap = 'gray', interpolation = 'bicubic')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Optic Disk\")\n",
    "        plt.show()\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Optic Cup\")\n",
    "        plt.show()\n",
    "        \n",
    "    if(disk_diameter == 0): return 1 # if disc not segmented properly then cdr might be infinity\n",
    "    cdr = cup_diameter/disk_diameter #ration of major axis of cup and disc\n",
    "    return cdr\n",
    "\n",
    "\n",
    "\n",
    "# MAIN FUNCTION\n",
    "CDR = [] # load calculated cdr here\n",
    "VAL = [] # load their labels here\n",
    "count = 0\n",
    "for i in range(len(X_all)):\n",
    "    set_path = os.path.join('E:\\\\drishti\\\\Drishti-GS1_files', 'Drishti-GS1_files','Drishti-GS1_files', 'Test',file_names[i])\n",
    "    image = cv2.imread(set_path,1)\n",
    "    segment(image,False,False)\n",
    "    cup = cv2.imread('C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/cup.png',0) #images will be saved in the same folder as the code so that folder needs to be put here\n",
    "    disc = cv2.imread('C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/disk.png',0) #images will be saved in the same folder as the code so that folder needs to be put here\n",
    "    cdr_cal = cdr(cup,disc,False,count)\n",
    "    if(val[1][int(file_codes_all[count][1:])-1] == 'Glaucomatous'):\n",
    "        VAL.append(1)\n",
    "    else:\n",
    "        VAL.append(0)\n",
    "    CDR.append(cdr_cal)\n",
    "    print(file_codes_all[count],'Exp1_cdr:',exp1[count],'Exp2_cdr:',exp2[count],'Exp3_cdr:',exp3[count],'Exp4_cdr:',exp4[count],'Pred_cdr:',cdr_cal)\n",
    "    os.remove('C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/cup.png') #put same folder as that of cup and disc so that they can be removed for processing of next set\n",
    "    os.remove('C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/disk.png')\n",
    "    count+=1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error1,error2,error3,error4 = [],[],[],[]\n",
    "#calculated error against each expert\n",
    "for i in range(len(X_all)):\n",
    "    error1.append(float(exp1[i]) - CDR[i])\n",
    "    error2.append(float(exp2[i]) - CDR[i])\n",
    "    error3.append(float(exp3[i]) - CDR[i])\n",
    "    error4.append(float(exp4[i]) - CDR[i])\n",
    "\n",
    "#saving the error and calculated cdr and its label into csv files for training classification model\n",
    "a = pd.DataFrame(error1) #exper1\n",
    "b = pd.DataFrame(error2) #exper2\n",
    "c = pd.DataFrame(error3) #exper3\n",
    "d = pd.DataFrame(error4) #exper4\n",
    "x1 = pd.DataFrame(CDR) #calculated cdr\n",
    "y1 = pd.DataFrame(VAL) #It's actual label\n",
    "x1.to_csv('C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/x.csv',index=False)\n",
    "y1.to_csv('C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/y.csv',index=False)\n",
    "a.to_csv('C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/a.csv',index=False)\n",
    "b.to_csv('C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/b.csv',index=False)\n",
    "c.to_csv('C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/c.csv',index=False)\n",
    "d.to_csv('C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/d.csv',index=False)\n",
    "\n",
    "\n",
    "# CLASSIFICATION MODEL\n",
    "\n",
    "#load all the required files\n",
    "X_train = pd.read_csv('C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/x.csv')\n",
    "Y_train = pd.read_csv('C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/y.csv')\n",
    "X_test = pd.read_csv('C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/x1.csv')\n",
    "Y_test = pd.read_csv('C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/y1.csv')\n",
    "a = pd.read_csv('C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/a.csv')\n",
    "b = pd.read_csv('C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/b.csv')\n",
    "c = pd.read_csv('C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/c.csv')\n",
    "d = pd.read_csv('C:\\\\Users\\\\deepak kumar\\\\Desktop\\\\BTP/d.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression() #train model with logistic regression\n",
    "\n",
    "logreg.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "acc = f1_score(Y_test, y_pred) #f1score for the classification\n",
    "print('Accuracy:',acc)\n",
    "print('Mean Error Expert1:',np.mean(a)[0],' ','STD Error Expert1:',np.std(a)[0])\n",
    "print('Mean Error Expert2:',np.mean(b)[0],' ','STD Error Expert2:',np.std(b)[0])\n",
    "print('Mean Error Expert3:',np.mean(c)[0],' ','STD Error Expert3:',np.std(c)[0])\n",
    "print('Mean Error Expert4:',np.mean(d)[0],' ','STD Error Expert4:',np.std(d)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
